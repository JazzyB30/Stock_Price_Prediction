# -*- coding: utf-8 -*-
"""Multiple Stock Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNRrkXYk_J2joP5A8fkTh4uu7azBxF9O
"""

!pip install pandas_ta

import pandas as pd
import pandas_ta as ta
import json
import requests

api_key = 'd482e4c93128c581fb6330925098dfa1'

def get_jsonparsed_data(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        raise ValueError(f"Failed to fetch data. Status code: {response.status_code}")

def get_historical_price(ticker):
    end_date = '2023-04-30'
    start_date = '2019-12-31'
    url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={start_date}&to={end_date}&apikey={api_key}'
    return get_jsonparsed_data(url)

def get_financial_ratios(ticker):
    url = f'https://financialmodelingprep.com/api/v3/ratios/{ticker}?apikey={api_key}'
    return get_jsonparsed_data(url)




# Change this path to the path of your CSV file
csv_file_path = '/content/drive/MyDrive/Sentiment Analysis/TickerStocks.csv'

# Read the CSV file using pandas
stocks_df = pd.read_csv(csv_file_path)




import os

# Create a directory to store the historical data
output_dir = '/content/drive/MyDrive/Sentiment Analysis/DataHistorical'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Iterate through the tickers and fetch historical data
for ticker in stocks_df['Ticker']:
    historical_data = get_historical_price(ticker)
    
    # Save the historical data to a JSON file
    file_path = os.path.join(output_dir, f"{ticker}_historical_data.json")
    with open(file_path, 'w') as f:
        json.dump(historical_data, f)

    print(f"Saved historical data for {ticker} to {file_path}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.layers import LSTM, Dense, Dropout

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.regularizers import L1L2
import pandas_ta as ta
import matplotlib.pyplot as plt

# Assuming historical_data is the fetched data
prices = pd.DataFrame(historical_data['historical'])
(prices.head(10))  # Display the first 10 rows

# Add 'actual_return' column
prices['actual_return'] = np.log(prices['close'] / prices['close'].shift(1))
prices.head()

# Preprocess the data: Use 'Date' as index and keep only 'Close' prices
prices['Date'] = pd.to_datetime(prices['date'])
prices = prices.set_index('Date').drop(columns=['date'])
prices = prices[['close','high','low','open','actual_return','change','changeOverTime','changePercent']]

import os
import csv

# Create a new directory to store the individual CSV files
directory = 'stock_predictions'
if not os.path.exists(directory):
    os.makedirs(directory)

def write_results_to_csv(directory, ticker, results, avg_price_diff):
    filename = f'{directory}/{ticker}_Real_predicted_prices.csv'
    file_exists = os.path.isfile(filename)

    with open(filename, 'a') as csvfile:
        headers = ['Ticker', 'Date', 'Actual', 'Predicted', 'Avg_Price_Diff']
        writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\n', fieldnames=headers)

        if not file_exists:
            writer.writeheader()

        for index, row in results.iterrows():
            writer.writerow({'Ticker': ticker, 'Date': index, 'Actual': row['Actual'], 'Predicted': row['Predicted'], 'Avg_Price_Diff': avg_price_diff})

def predict_prices(ticker, historical_data):
    
 if 'historical' not in historical_data:
        print(f"Error: No historical data found for {ticker}")
        return

 prices = pd.DataFrame(historical_data['historical'])
    
 if 'date' not in prices.columns:
        print(f"Error: 'date' column not found in historical data for {ticker}")
        return
 # Assuming historical_data is the fetched data
 prices = pd.DataFrame(historical_data['historical'])
 (prices.head(10))  # Display the first 10 rows
 
 # Add 'actual_return' column
 prices['actual_return'] = np.log(prices['close'] / prices['close'].shift(1))
    
    # Preprocess the data: Use 'Date' as index and keep only 'Close' prices
 prices['Date'] = pd.to_datetime(prices['date'])
 prices = prices.set_index('Date').drop(columns=['date'])
 prices = prices[['close','high','low','open','actual_return','change','changeOverTime','changePercent','volume']]#   # Preprocess the data: Use 'Date' as index and keep only 'Close' prices
#  prices['Date'] = pd.to_datetime(prices['date'])
#  prices = prices.set_index('Date').drop(columns=['date'])
#  prices = prices[['close','high','low','open','actual_return']]



# Calculate the moving averages (short and long windows)
 short_window = 5
 long_window = 20
 prices['short_mavg'] = prices['close'].rolling(window=short_window).mean()
 prices['long_mavg'] = prices['close'].rolling(window=long_window).mean()

# Calculate the RSI
 rsi_period = 14
 prices['rsi'] = ta.rsi(prices['close'], length=rsi_period)

# Calculate the Bollinger Bands
 bb_period = 20
 bbands = ta.bbands(prices['close'], length=bb_period)
 prices['bb_lower'] = bbands.iloc[:, 0]
 prices['bb_middle'] = bbands.iloc[:, 1]
 prices['bb_upper'] = bbands.iloc[:, 2]

 # # Additional features for volatile stocks
 prices['atr'] = ta.atr(prices['high'], prices['low'], prices['close'], length=14)
 prices['roc'] = ta.roc(prices['close'], length=10)
# parabolic_sar_df = ta.psar(prices['high'], prices['low'], prices['close'])
# prices['parabolic_sar_s'] = parabolic_sar_df['PSARs_0.02_0.2']
# prices['parabolic_sar_l'] = parabolic_sar_df['PSARl_0.02_0.2']

 prices['macd'] = ta.macd(prices['close'], fast=12, slow=26, signal=9).iloc[:, 0]
# # prices['fibonacci_retracements'] = ta.fibonacci_retracements(prices['close'])
 prices['cmf'] = ta.cmf(prices['high'], prices['low'], prices['close'], prices['volume'], length=20)

# Additional features for non-volatile stocks
 prices['sma'] = ta.sma(prices['close'], length=20)
 prices['ema'] = ta.ema(prices['close'], length=20)
 prices['obv'] = ta.obv(prices['close'], prices['volume'])


#  # Calculate the ATR (Average True Range) for Volatility
#  atr_period = 14
#  prices['atr'] = ta.atr(prices['high'], prices['low'], prices['close'], length=atr_period)

#  # Calculate the MACD (Moving Average Convergence Divergence) for Momentum
#  macd_short_period = 12
#  macd_long_period = 26
#  macd_signal_period = 9
#  macd = ta.macd(prices['close'], fast=macd_short_period, slow=macd_long_period, signal=macd_signal_period)
#  prices['macd'] = macd.iloc[:, 0]
#  prices['macd_signal'] = macd.iloc[:, 1]
 

# Drop the rows with NaN values generated due to the rolling window calculations
 prices = prices.dropna()
 
 prices = prices.sort_index()


 # Keep only the relevant features
#  prices = prices[['close', 'short_mavg', 'long_mavg', 'rsi', 'bb_lower', 'bb_middle', 'bb_upper']]
#  prices = prices[['open','high','low','close']]

 prices = prices.sort_index()

 def create_features_target(prices, window_size=30, selected_features=None):
    if selected_features is None:
        selected_features = prices.columns

    features = []
    target = []
    for i in range(len(prices) - window_size):
        window_data = prices[selected_features].iloc[i:i + window_size]
        features.append(window_data.values)
        target.append(prices.iloc[i + window_size]['close'])

    return np.array(features), np.array(target)
 selected_features = [
    'high', 'low', 'short_mavg', 'long_mavg', 'close', 'actual_return',
    'atr', 'roc',  'macd', 'cmf','open',
    'sma', 'ema', 'obv','change','changePercent','changeOverTime'
 ] 
 window_size = 2

 X, y = create_features_target(prices, window_size=window_size, selected_features=selected_features)


# Train-test split
 test_size = 0.370  # Decrease the test dataset size
 split_index = int(len(X) * (1 - test_size))
 X_train, X_test = X[:split_index], X[split_index:]
 y_train, y_test = y[:split_index], y[split_index:]

# Check the first and last dates in the test dataset
 print(f"First date in test dataset: {prices.index[-len(y_test)]}")
 print(f"Last date in test dataset: {prices.index[-1]}")

# Scale the data
 scaler_X = MinMaxScaler()
 scaler_y = MinMaxScaler()

 X_train = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(-1, window_size, X_train.shape[2])
 X_test = scaler_X.transform(X_test.reshape(-1, X_test.shape[2])).reshape(-1, window_size, X_test.shape[2])

 y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))
 y_test = scaler_y.transform(y_test.reshape(-1, 1))

#  # Build the GRU model with L1 and L2 regularization
#  model = Sequential([
#       GRU(32, activation='relu', input_shape=(window_size, 7),
#           kernel_regularizer=L1L2(l1=0.001, l2=0.001), recurrent_regularizer=L1L2(l1=0.001, l2=0.001)),
#       Dropout(0.2),
#       Dense(1)
#  )]
# # model = Sequential()
#  model.add(LSTM(units=50, return_sequences=True, input_shape=(window_size, 7),
#                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
#                    bias_regularizer=regularizers.l2(1e-4),
#                    activity_regularizer=regularizers.l2(1e-5)))
#  model.add(Dropout(0.2))
#  model.add(LSTM(units=50, return_sequences=True,
#                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
#                    bias_regularizer=regularizers.l2(1e-4),
#                    activity_regularizer=regularizers.l2(1e-5)))
#  model.add(Dropout(0.2))
#  model.add(LSTM(units=50, return_sequences=False,
#                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
#                    bias_regularizer=regularizers.l2(1e-4),
#                    activity_regularizer=regularizers.l2(1e-5)))
#  model.add(Dropout(0.2))
#  model.add(Dense(units=1))

 from tensorflow.keras.layers import LSTM

# Build the LSTM model with L2 regularization
 model = Sequential([
    LSTM(32, activation='relu', input_shape=(window_size, len(selected_features)),
         kernel_regularizer=L1L2(l2=0.001), recurrent_regularizer=L1L2(l2=0.001,l1=0.001)),
    Dropout(0.2),
    Dense(1)
 ])

    



# Compile and train the GRU model
 model.compile(optimizer='adam', loss='mean_squared_error')

# Add early stopping and model checkpoints
 early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
 model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

 model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.1, callbacks=[early_stopping, model_checkpoint])

# Evaluate the model
 train_loss = model.evaluate(X_train, y_train)
 test_loss = model.evaluate(X_test, y_test)
 print(f'Train Loss: {train_loss:.4f}')
 print(f'Test Loss: {test_loss:.4f}')

# Make predictions
 y_pred = model.predict(X_test)
 y_pred = scaler_y.inverse_transform(y_pred)  # Convert predictions back to the original scale
 y_test_original = scaler_y.inverse_transform(y_test)

 test_dates = prices.index[-len(y_test):]

 results = pd.DataFrame({'Date': test_dates, 'Actual': y_test_original.flatten(), 'Predicted': y_pred.flatten()})
 results = results.set_index('Date')

 
  


 # Calculate the average price difference
 avg_price_diff = (results['Actual'] - results['Predicted']).mean()

 return results, avg_price_diff

# Replace the call to `write_results_to_csv` with the modified version
for ticker in stocks_df['Ticker']:
    print(f'Processing ticker: {ticker}')
    historical_data = get_historical_price(ticker)
    results, avg_price_diff = predict_prices(ticker, historical_data)
    write_results_to_csv(directory, ticker, results, avg_price_diff)  # Pass the avg_price_diff as an argument
    print('\n' + '-' * 80 + '\n')

# Function to get SPY historical price data
def get_spy_performance(start_date, end_date):
    ticker = 'SPY'
    return get_historical_price(ticker, start_date, end_date)

# Function to calculate the SPY returns
def calculate_spy_returns(spy_data):
    spy_prices = pd.DataFrame(spy_data['historical'])
    spy_prices['Date'] = pd.to_datetime(spy_prices['date'])
    spy_prices = spy_prices.set_index('Date').drop(columns=['date'])
    spy_prices = spy_prices[['close']]
    
    spy_returns = spy_prices.pct_change().dropna()
    return spy_returns

def get_historical_price(ticker, start_date, end_date):
    url = f'https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={start_date}&to={end_date}&apikey={api_key}'
    return get_jsonparsed_data(url)

# Retrieve SPY historical price data for the same dates
start_date = '2019-12-31'
end_date = '2023-03-03'
spy_data = get_historical_price('SPY', start_date, end_date)

# Calculate the SPY returns
spy_returns = calculate_spy_returns(spy_data)

# Calculate the stock's daily returns
stock_returns = prices['close'].pct_change().dropna()

# Get the test dataset dates
test_dates = prices.index[-len(y_test):]

# Calculate the stock's returns for the test dataset
test_stock_returns = stock_returns.loc[test_dates]

# Calculate the SPY returns for the test dataset
test_spy_returns = spy_returns.loc[test_dates]

# Calculate the stock's predicted returns
results['Predicted_Returns'] = results['Predicted'].pct_change().dropna()

# Calculate the stock's and SPY's cumulative returns for the test dataset
cumulative_stock_returns = (1 + test_stock_returns).cumprod() - 1
cumulative_spy_returns = (1 + test_spy_returns).cumprod() - 1
cumulative_predicted_returns = (1 + results['Predicted_Returns']).cumprod() - 1

# Plot the stock's and SPY's cumulative returns
plt.figure(figsize=(12, 6))
plt.plot(cumulative_stock_returns, label='Stock Actual Returns')
plt.plot(cumulative_predicted_returns, label='Stock Predicted Returns')
plt.plot(cumulative_spy_returns, label='SPY Returns')
plt.title('Stock and SPY Cumulative Returns')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(results['Actual'], label='Actual')
plt.plot(results['Predicted'], label='Predicted')
plt.title('Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.show()

lag_days = 5  # Change this value to set the number of days to shift the predicted values

results['Predicted_Shifted'] = results['Predicted'].shift(lag_days)

plt.figure(figsize=(12, 6))
plt.plot(results['Actual'], label='Actual')
plt.plot(results['Predicted_Shifted'], label=f'Predicted (shifted by {lag_days} days)')
plt.title('Stock Price Prediction with Lag')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.show()